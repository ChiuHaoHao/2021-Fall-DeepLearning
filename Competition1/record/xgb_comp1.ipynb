{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier,  AdaBoostClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from xgboost.sklearn import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jinghao/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "def preprocessor(text):\n",
    "    # remove HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    \n",
    "    # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n",
    "    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    emoticons = re.findall(r, text)\n",
    "    text = re.sub(r, '', text)\n",
    "    \n",
    "    # convert to lowercase and append all emoticons behind (with space in between)\n",
    "    # replace('-','') removes nose of emoticons\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "    return text\n",
    "\n",
    "def tokenizer_stem(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(word) for word in re.split('\\s+', text.strip())]\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer_stem_nostop(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]\n",
    "\n",
    "def get_stream(path, size):\n",
    "    for chunk in pd.read_csv(path, chunksize=size):\n",
    "        yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocessor_tag(text, tag):\n",
    "#     ts = BeautifulSoup(text, 'html.parser').find_all(tag)\n",
    "#     text = \"\"\n",
    "#     for t in ts:\n",
    "#         text += t.get_text()+ \" \"\n",
    "#     r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "#     emoticons = re.findall(r, text)\n",
    "#     text = re.sub(r, '', text)\n",
    "    \n",
    "#     # convert to lowercase and append all emoticons behind (with space in between)\n",
    "#     # replace('-','') removes nose of emoticons\n",
    "#     text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "#     return text\n",
    "\n",
    "def preprocessor_tag(text):\n",
    "    ts = BeautifulSoup(text, 'html.parser').find_all( attrs={\"class\": \"author_name\"})\n",
    "    #print(ts)\n",
    "    text = \"\"\n",
    "    for t in ts:\n",
    "        text += t.get_text()+ \" \"\n",
    "    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    emoticons = re.findall(r, text)\n",
    "    text = re.sub(r, '', text)\n",
    "    print(text)\n",
    "    # convert to lowercase and append all emoticons behind (with space in between)\n",
    "    # replace('-','') removes nose of emoticons\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocessor_tag_general(text, tag):\n",
    "#     ts = BeautifulSoup(text, 'html.parser').find_all(tag)\n",
    "#     text = \"\"\n",
    "#     for t in ts:\n",
    "#         text += t.get_text()+ \" \"\n",
    "#     r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "#     emoticons = re.findall(r, text)\n",
    "#     text = re.sub(r, '', text)\n",
    "    \n",
    "#     # convert to lowercase and append all emoticons behind (with space in between)\n",
    "#     # replace('-','') removes nose of emoticons\n",
    "#     text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "#     return text\n",
    "def preprocessor_tag_general(text):\n",
    "    ts = BeautifulSoup(text, 'html.parser').find_all('p')\n",
    "    text = \"\"\n",
    "    for t in ts:\n",
    "        text += t.get_text()+ \" \"\n",
    "    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    emoticons = re.findall(r, text)\n",
    "    text = re.sub(r, '', text)\n",
    "    \n",
    "    # convert to lowercase and append all emoticons behind (with space in between)\n",
    "    # replace('-','') removes nose of emoticons\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "    return text\n",
    "\n",
    "def preprocessor_tag(text):\n",
    "    ts = BeautifulSoup(text, 'html.parser').find_all( attrs={\"class\": \"author_name\"})\n",
    "    #print(ts)\n",
    "    text = \"\"\n",
    "    for t in ts:\n",
    "        text += t.get_text()+ \" \"\n",
    "    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    emoticons = re.findall(r, text)\n",
    "    text = re.sub(r, '', text)\n",
    "    # print(text)\n",
    "    # convert to lowercase and append all emoticons behind (with space in between)\n",
    "    # replace('-','') removes nose of emoticons\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000/25000] 0.5127499959968616\n",
      "[4000/25000] 0.5258887806598763\n",
      "[6000/25000] 0.5201500806003224\n",
      "[8000/25000] 0.5232613985655739\n",
      "[10000/25000] 0.5455670934570017\n",
      "[12000/25000] 0.48864959138528985\n",
      "[14000/25000] 0.5294155306319247\n",
      "[16000/25000] 0.5215907772679816\n",
      "[18000/25000] 0.5280995075034707\n",
      "[20000/25000] 0.521439661256175\n",
      "[22000/25000] 0.5373575179303279\n",
      "[24000/25000] 0.5677330837293397\n"
     ]
    }
   ],
   "source": [
    "hashvec_author = HashingVectorizer(n_features=2**20, \n",
    "                            preprocessor=preprocessor_tag, tokenizer=tokenizer_stem_nostop)\n",
    "\n",
    "hashvec = HashingVectorizer(n_features=2**20, \n",
    "                            preprocessor=preprocessor, tokenizer=tokenizer_stem_nostop)\n",
    "\n",
    "hashvec_paragraph = HashingVectorizer(n_features=2**20, \n",
    "                            preprocessor=preprocessor_tag_general, tokenizer=tokenizer_stem_nostop)\n",
    "                            \n",
    "# loss='log' gives logistic regression\n",
    "# clf_author = SGDClassifier(loss='log', max_iter=100, tol=1e-3)\n",
    "# clf_paragraph = SGDClassifier(loss='log', max_iter=100, tol=1e-3)\n",
    "# clf = SGDClassifier(loss='log', max_iter=100, tol=1e-3)\n",
    "clf_author = SGDClassifier(loss='log', max_iter=100, tol=1e-3)\n",
    "clf_paragraph = SGDClassifier(loss='log', max_iter=100, tol=1e-3)\n",
    "clf = SGDClassifier(loss='log', max_iter=100, tol=1e-3)\n",
    "\n",
    "batch_size = 1000\n",
    "stream = get_stream(path='/home/jinghao/miniconda3/DL_comp1/train.csv', size=batch_size)\n",
    "classes = np.array([-1, 1])\n",
    "train_auc, val_auc = [], []\n",
    "# we use one batch for training and another for validation in each iteration\n",
    "iters = int((25000+batch_size-1)/(batch_size*2))\n",
    "for i in range(iters):\n",
    "    batch = next(stream)\n",
    "    X_train, y_train = batch['Page content'], batch['Popularity']\n",
    "    if X_train is None:\n",
    "        break\n",
    "\n",
    "    X_train_author = hashvec_author.transform(X_train)\n",
    "    clf_author.partial_fit(X_train_author, y_train, classes=classes)\n",
    "\n",
    "    X_train_paragraph = hashvec_paragraph.transform(X_train)\n",
    "    clf_paragraph.partial_fit(X_train_paragraph, y_train, classes=classes)\n",
    "\n",
    "    X_train_original = hashvec.transform(X_train)\n",
    "    clf.partial_fit(X_train_original, y_train, classes=classes)\n",
    "\n",
    "    author = clf_author.predict_proba(X_train_author)[:,1]\n",
    "    paragraph = clf_paragraph.predict_proba(X_train_paragraph)[:,1]\n",
    "    original = clf.predict_proba(X_train_original)[:,1]\n",
    "    vote_train = (0.4*author+0.3*paragraph+0.3*original)\n",
    "    \n",
    "    train_auc.append(roc_auc_score(y_train, vote_train))\n",
    "    \n",
    "    # validate\n",
    "    batch = next(stream)\n",
    "    X_val, y = batch['Page content'], batch['Popularity']\n",
    "\n",
    "    X_val_author = hashvec_author.transform(X_val)\n",
    "\n",
    "    X_val_paragraph = hashvec_paragraph.transform(X_val)\n",
    "\n",
    "    X_val_original = hashvec.transform(X_val)\n",
    "\n",
    "    author = clf_author.predict_proba(X_val_author)[:,1]\n",
    "    paragraph = clf_paragraph.predict_proba(X_val_paragraph)[:,1]\n",
    "    original = clf.predict_proba(X_val_original)[:,1]\n",
    "    vote_val = (0.4*author+0.3*paragraph+0.3*original)\n",
    "\n",
    "    score = roc_auc_score(y, vote_val)\n",
    "\n",
    "    val_auc.append(score)\n",
    "    print('[{}/{}] {}'.format((i+1)*(batch_size*2), 25000, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optimized pickle written in C for serializing and \n",
    "# de-serializing a Python object\n",
    "import _pickle as pkl\n",
    "\n",
    "# dump to disk\n",
    "pkl.dump(hashvec_author, open('/home/jinghao/miniconda3/DL_comp1/output/hashvec_author.pkl', 'wb'))\n",
    "pkl.dump(hashvec_paragraph, open('/home/jinghao/miniconda3/DL_comp1/output/hashvec_paragraph.pkl', 'wb'))\n",
    "pkl.dump(hashvec, open('/home/jinghao/miniconda3/DL_comp1/output/hashvec_original.pkl', 'wb'))\n",
    "\n",
    "pkl.dump(clf_author, open('/home/jinghao/miniconda3/DL_comp1/output/clf_author.pkl', 'wb'))\n",
    "pkl.dump(clf_paragraph, open('/home/jinghao/miniconda3/DL_comp1/output/clf_paragraph.pkl', 'wb'))\n",
    "pkl.dump(clf, open('/home/jinghao/miniconda3/DL_comp1/output/clf_original.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from disk\n",
    "hashvec_author = pkl.load(open('/home/jinghao/miniconda3/DL_comp1/output/hashvec_author.pkl', 'rb'))\n",
    "clf_author = pkl.load(open('/home/jinghao/miniconda3/DL_comp1/output/clf_author.pkl', 'rb'))\n",
    "\n",
    "hashvec_paragraph = pkl.load(open('/home/jinghao/miniconda3/DL_comp1/output/hashvec_paragraph.pkl', 'rb'))\n",
    "clf_paragraph = pkl.load(open('/home/jinghao/miniconda3/DL_comp1/output/clf_paragraph.pkl', 'rb'))\n",
    "\n",
    "hashvec_original= pkl.load(open('/home/jinghao/miniconda3/DL_comp1/output/hashvec_original.pkl', 'rb'))\n",
    "clf_original = pkl.load(open('/home/jinghao/miniconda3/DL_comp1/output/clf_original.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('/home/jinghao/miniconda3/DL_comp1/test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hashvec = HashingVectorizer(n_features=2**20, \n",
    "#                             preprocessor=preprocessor, tokenizer=tokenizer_stem_nostop)\n",
    "\n",
    "# # loss='log' gives logistic regression\n",
    "# clf = SGDClassifier(loss='log', max_iter=100, tol=1e-3)\n",
    "# batch_size = 1000\n",
    "# stream = get_stream(path='/home/jinghao/miniconda3/DL_comp1/train.csv', size=batch_size)\n",
    "# classes = np.array([-1, 1])\n",
    "# train_auc, val_auc = [], []\n",
    "# # we use one batch for training and another for validation in each iteration\n",
    "# iters = int((25000+batch_size-1)/(batch_size*2))\n",
    "\n",
    "\n",
    "# img_tag = []\n",
    "# href_tag = []\n",
    "\n",
    "\n",
    "# for i in range(iters):\n",
    "#     paragraph_train =[]\n",
    "#     paragraph_val =[]\n",
    "#     batch = next(stream)\n",
    "#     X_train, y_train = batch['Page content'], batch['Popularity']\n",
    "#     if X_train is None:\n",
    "#         break\n",
    "    \n",
    "#     # preprocess the X_train as only paragraph part\n",
    "#     for page in X_train:\n",
    "#         soup = BeautifulSoup(page)\n",
    "#         # img_tag.append(len(soup.select('img')))\n",
    "#         # href_tag.append(len(soup.find_all('a', href=True)))\n",
    "#         paragraph_text = ''\n",
    "#         for p in soup.find_all('p'):\n",
    "#             paragraph_text = paragraph_text + p.get_text()\n",
    "#         paragraph_train.append(paragraph_text)\n",
    "    \n",
    "#     X_train = hashvec.transform(paragraph_train)\n",
    "#     # X_train = tfidf.fit_transform(paragraph_train)\n",
    "#     # ch2 = SelectKBest(chi2)\n",
    "#     # X_train = ch2.fit_transform(X_train, y_train)\n",
    "#     clf.partial_fit(X_train, y_train, classes=classes)\n",
    "\n",
    "#     score = roc_auc_score(y_train, clf.predict_proba(X_train)[:,1])\n",
    "#     train_auc.append(score)\n",
    "#     print('[{}/{}] train score: {}'.format((i+1)*(batch_size*2), 25000, score))\n",
    "#     # validate\n",
    "#     batch = next(stream)\n",
    "#     X_val, y_val = batch['Page content'], batch['Popularity']\n",
    "    \n",
    "#     # preprocess the X_val as only paragraph part\n",
    "#     for page in X_val:\n",
    "#         soup = BeautifulSoup(page)\n",
    "#         # img_tag.append(len(soup.select('img')))\n",
    "#         # href_tag.append(len(soup.find_all('a', href=True)))\n",
    "#         paragraph_text = ''\n",
    "#         for p in soup.find_all('p'):\n",
    "#             paragraph_text = paragraph_text + p.get_text()\n",
    "#         paragraph_val.append(paragraph_text)\n",
    "    \n",
    "#     score = roc_auc_score(y_val, clf.predict_proba(hashvec.transform(paragraph_val))[:,1])\n",
    "#     val_auc.append(score)\n",
    "#     print('[{}/{}] val score: {}\\n'.format((i+1)*(batch_size*2), 25000, score))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "54b8a75404ab4e6ba29309be618ca70d85c3d31f801fa89aae55b1caf37627da"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('pytorch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
