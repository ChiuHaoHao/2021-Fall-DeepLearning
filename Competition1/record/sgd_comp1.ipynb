{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier,  AdaBoostClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jinghao/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "def preprocessor(text):\n",
    "    # remove HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    \n",
    "    # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n",
    "    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    emoticons = re.findall(r, text)\n",
    "    text = re.sub(r, '', text)\n",
    "    \n",
    "    # convert to lowercase and append all emoticons behind (with space in between)\n",
    "    # replace('-','') removes nose of emoticons\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "    return text\n",
    "\n",
    "def tokenizer_stem(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(word) for word in re.split('\\s+', text.strip())]\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer_stem_nostop(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]\n",
    "\n",
    "def get_stream(path, size):\n",
    "    for chunk in pd.read_csv(path, chunksize=size):\n",
    "        yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000/25000] train score: 0.9422412308628775\n",
      "[2000/25000] val score: 0.4964292004931867\n",
      "[4000/25000] train score: 0.959603643278951\n",
      "[4000/25000] val score: 0.5367517667384185\n",
      "[6000/25000] train score: 0.9427157708630834\n",
      "[6000/25000] val score: 0.5075780303121212\n",
      "[8000/25000] train score: 0.9349405112074244\n",
      "[8000/25000] val score: 0.520936059170082\n",
      "[10000/25000] train score: 0.9131110256903068\n",
      "[10000/25000] val score: 0.5100352858151023\n",
      "[12000/25000] train score: 0.9127711634649655\n",
      "[12000/25000] val score: 0.4941377889604026\n",
      "[14000/25000] train score: 0.9062116248464994\n",
      "[14000/25000] val score: 0.5282431507808529\n",
      "[16000/25000] train score: 0.8763761660643752\n",
      "[16000/25000] val score: 0.5007580272889824\n",
      "[18000/25000] train score: 0.8887718483154167\n",
      "[18000/25000] val score: 0.5239026849262455\n",
      "[20000/25000] train score: 0.8672649938580099\n",
      "[20000/25000] val score: 0.5025425950196593\n",
      "[22000/25000] train score: 0.8397494359909761\n",
      "[22000/25000] val score: 0.5383981173155739\n",
      "[24000/25000] train score: 0.8635514542058168\n",
      "[24000/25000] val score: 0.5344245507928127\n"
     ]
    }
   ],
   "source": [
    "hashvec = HashingVectorizer(n_features=2**20, \n",
    "                            preprocessor=preprocessor, tokenizer=tokenizer_stem_nostop)\n",
    "\n",
    "# loss='log' gives logistic regression\n",
    "clf = SGDClassifier(loss='log', max_iter=100, tol=1e-3)\n",
    "batch_size = 1000\n",
    "stream = get_stream(path='/home/jinghao/miniconda3/DL_comp1/train.csv', size=batch_size)\n",
    "classes = np.array([-1, 1])\n",
    "train_auc, val_auc = [], []\n",
    "# we use one batch for training and another for validation in each iteration\n",
    "iters = int((25000+batch_size-1)/(batch_size*2))\n",
    "\n",
    "\n",
    "img_tag = []\n",
    "href_tag = []\n",
    "\n",
    "\n",
    "for i in range(iters):\n",
    "    paragraph_train =[]\n",
    "    paragraph_val =[]\n",
    "    batch = next(stream)\n",
    "    X_train, y_train = batch['Page content'], batch['Popularity']\n",
    "    if X_train is None:\n",
    "        break\n",
    "    \n",
    "    # preprocess the X_train as only paragraph part\n",
    "    for page in X_train:\n",
    "        soup = BeautifulSoup(page)\n",
    "        # img_tag.append(len(soup.select('img')))\n",
    "        # href_tag.append(len(soup.find_all('a', href=True)))\n",
    "        paragraph_text = ''\n",
    "        for p in soup.find_all('p'):\n",
    "            paragraph_text = paragraph_text + p.get_text()\n",
    "        paragraph_train.append(paragraph_text)\n",
    "    \n",
    "    X_train = hashvec.transform(paragraph_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    score = roc_auc_score(y_train, clf.predict_proba(X_train)[:,1])\n",
    "    train_auc.append(score)\n",
    "    print('[{}/{}] train score: {}'.format((i+1)*(batch_size*2), 25000, score))\n",
    "    # validate\n",
    "    batch = next(stream)\n",
    "    X_val, y_val = batch['Page content'], batch['Popularity']\n",
    "    \n",
    "    # preprocess the X_val as only paragraph part\n",
    "    for page in X_val:\n",
    "        soup = BeautifulSoup(page)\n",
    "        # img_tag.append(len(soup.select('img')))\n",
    "        # href_tag.append(len(soup.find_all('a', href=True)))\n",
    "        paragraph_text = ''\n",
    "        for p in soup.find_all('p'):\n",
    "            paragraph_text = paragraph_text + p.get_text()\n",
    "        paragraph_val.append(paragraph_text)\n",
    "    \n",
    "    score = roc_auc_score(y_val, clf.predict_proba(hashvec.transform(paragraph_val))[:,1])\n",
    "    val_auc.append(score)\n",
    "    print('[{}/{}] val score: {}'.format((i+1)*(batch_size*2), 25000, score))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "54b8a75404ab4e6ba29309be618ca70d85c3d31f801fa89aae55b1caf37627da"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('pytorch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
