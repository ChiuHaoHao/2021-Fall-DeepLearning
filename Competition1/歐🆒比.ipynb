{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Members\n",
    "* 107062208邱靖豪 \n",
    "* 107062132鍾皓崴 \n",
    "* 107061202蔣承軒 \n",
    "* 110062577景璞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier,  AdaBoostClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This function is used to generate the dataset from csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stream(path, size):\n",
    "    for chunk in pd.read_csv(path, chunksize=size):\n",
    "        yield chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTML Preprocessor\n",
    "We preprocessed some features from original HTML file. Include\n",
    "* Time\n",
    "* Category\n",
    "* Data Channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conver time from string to integer, make it possible to be useful feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor_time(text):\n",
    "    ts = BeautifulSoup(text, 'html.parser').find_all('time')\n",
    "    text = \"\"\n",
    "    for t in ts:\n",
    "        text += t.get_text()+ \" \"\n",
    "    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    emoticons = re.findall(r, text)\n",
    "    text = re.sub(r, '', text)\n",
    "    # convert to lowercase and append all emoticons behind (with space in between)\n",
    "    # replace('-','') removes nose of emoticons\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "    return text\n",
    "\n",
    "def generate_time_num_feature(raw_text):\n",
    "    feature = []\n",
    "    for r_t in raw_text:\n",
    "        time_text = preprocessor_time(r_t).split(' ')\n",
    "        f = []\n",
    "        for i in range(4):\n",
    "            try:\n",
    "                time = int(time_text[i])\n",
    "            except:\n",
    "                time = 0\n",
    "            f.append(time)\n",
    "        try:\n",
    "            temp = pd.Timestamp(time_text[0]+'-'+time_text[1]+'-'+time_text[2])\n",
    "            f.append(temp.dayofweek)\n",
    "        except:\n",
    "            f.append(-1)\n",
    "        f = np.array(f)\n",
    "        feature.append(f)\n",
    "    feature = np.array(feature)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This part is used to generate the frequency of specific category\n",
    "* Find out 5 mostly important features and encode them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequency(text_, y):\n",
    "    cnt = 1\n",
    "    for label, i in tqdm(zip(y, text_)):\n",
    "        soup = BeautifulSoup(i)\n",
    "        for link in soup.find_all('a'):\n",
    "            t = link.get('href')\n",
    "            if t != None:\n",
    "                if t.find('category') != -1 and t.find('http') == -1:\n",
    "                    for idx, i in enumerate(t.split('/')):\n",
    "                        if i == 'category': \n",
    "                            tag = t.split('/')[idx+1]\n",
    "                            if tag not in frequency:\n",
    "                                dict_[tag] = cnt\n",
    "                                frequency[tag] = 1\n",
    "                                average_label[tag] = label\n",
    "                                cnt += 1\n",
    "                            else:\n",
    "                                frequency[tag] += 1\n",
    "                                average_label[tag] += label \n",
    "\n",
    "\n",
    "def preprocessor_category_frequency(text_):\n",
    "    soup = BeautifulSoup(text_)\n",
    "    text = []\n",
    "    for link in soup.find_all('a'):\n",
    "        t = link.get('href')\n",
    "        if t != None:\n",
    "            if t.find('category') != -1 and t.find('http') == -1:\n",
    "                for idx, i in enumerate(t.split('/')):\n",
    "                    if i == 'category':\n",
    "                        tag = t.split('/')[idx+1]\n",
    "                        if tag in frequency and abs(average_label[tag]) > 50:\n",
    "                            text.append(average_label[tag])\n",
    "    return text, len(text)\n",
    "\n",
    "\n",
    "def generate_cate_num_feature(raw_text):\n",
    "    feature = []\n",
    "    max_len = -1\n",
    "    for r_t in tqdm(raw_text):\n",
    "        href_num, len_ = preprocessor_category_frequency(r_t)\n",
    "        if max_len < len_:\n",
    "            max_len = len_\n",
    "        feature.append(href_num[0:5])\n",
    "    for idx, r_t in enumerate(feature):\n",
    "        for i in range(0, 5 - len(r_t)):\n",
    "            feature[idx].append(0)\n",
    "    feature = np.array(feature)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the data channel\n",
    "* Since there are only one data channel tag in each news, so used target embedding to encode it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_datachannel(text_):\n",
    "    soup = BeautifulSoup(text_)\n",
    "    pa = soup.find('article')\n",
    "    text = pa.get('data-channel')\n",
    "    if text == None:\n",
    "        text = 'None'\n",
    "    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    emoticons = re.findall(r, text)\n",
    "    text = re.sub(r, '', text)\n",
    "    # convert to lowercase and append all emoticons behind (with space in between)\n",
    "    # replace('-','') removes nose of emoticons\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "    return text \n",
    "\n",
    "def get_datachannel_frequency(raw_text, y):\n",
    "    feature = []\n",
    "    for r_t in tqdm(raw_text):\n",
    "        img_num = preprocess_datachannel(r_t)\n",
    "        feature.append(img_num)\n",
    "    for tag, label in tqdm(zip(feature, y)):\n",
    "        if tag not in frequency_data_channel:\n",
    "            frequency_data_channel[tag] = 1\n",
    "            data_channel_average[tag] = label\n",
    "        else:\n",
    "            frequency_data_channel[tag] += 1\n",
    "            data_channel_average[tag] += label\n",
    "\n",
    "\n",
    "def generate_datachannel(raw_text):\n",
    "    out = []\n",
    "    for r_t in raw_text:\n",
    "        text = preprocess_datachannel(r_t)\n",
    "        if text in frequency_data_channel:\n",
    "            img_num = data_channel_average[text]\n",
    "            out.append(img_num)   \n",
    "        else:\n",
    "            out.append(-1)\n",
    "    feature = np.array(out)\n",
    "    feature = feature.reshape(feature.shape[0], 1)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "* Using XGBClassifier as classifier\n",
    "* Take time, category, channel as feature\n",
    "* Concate all features to train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class numerical_estimator():\n",
    "    def __init__(self):\n",
    "        self.pipes = []\n",
    "        self.pipes.append(generate_time_num_feature)\n",
    "        self.pipes.append(generate_cate_num_feature)\n",
    "        self.pipes.append(generate_datachannel)\n",
    "        self.estimator = XGBClassifier(n_estimators=5000, max_depth=5, gamma=6, objective='binary:logistic', eval_metric='auc')#RandomForestClassifier(max_depth=8, random_state=0, min_samples_leaf=10,min_samples_split=20, n_estimators=1000)\n",
    "        self.scaler = StandardScaler()\n",
    "    def fit(self, X, Y):\n",
    "        \n",
    "        self.features = []\n",
    "        for p in self.pipes:\n",
    "            self.features.append(p(X))\n",
    "        self.all_feature = np.concatenate(self.features, axis=1)\n",
    "        print(self.all_feature.shape)\n",
    "        self.scaler.fit(self.all_feature)\n",
    "        self.all_feature=self.scaler.transform(self.all_feature)\n",
    "        self.estimator.fit(self.all_feature, Y)\n",
    "    def predict(self, X):\n",
    "        self.features = []\n",
    "        for p in self.pipes:\n",
    "            self.features.append(p(X))\n",
    "        self.all_feature = np.concatenate(self.features, axis=1)\n",
    "        self.all_feature=self.scaler.transform(self.all_feature)\n",
    "        print(self.all_feature.shape)\n",
    "        return self.estimator.predict(self.all_feature)\n",
    "    def predict_proba(self, X):\n",
    "        self.features = []\n",
    "        for p in self.pipes:\n",
    "            self.features.append(p(X))\n",
    "        self.all_feature = np.concatenate(self.features, axis=1)\n",
    "        self.all_feature=self.scaler.transform(self.all_feature)\n",
    "        print(self.all_feature.shape)\n",
    "        return self.estimator.predict_proba(self.all_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Process\n",
    "* Use 22000 data as training data, other are validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = get_stream(path='./train.csv', size=22000)\n",
    "classes = np.array([-1, 1])\n",
    "batch = next(stream)\n",
    "X_train, y_train = batch['Page content'], batch['Popularity']\n",
    "\n",
    "average_label = {}\n",
    "data_channel_average = {}\n",
    "frequency = {}\n",
    "frequency_data_channel = {}\n",
    "dict_ = {}\n",
    "get_frequency(X_train, y_train)\n",
    "get_datachannel_frequency(X_train, y_train)\n",
    "\n",
    "model = numerical_estimator()\n",
    "model.fit(X_train, y_train)\n",
    "score = roc_auc_score(y_train, model.predict_proba(X_train)[:,1])\n",
    "\n",
    "print('train: {}'.format(score))\n",
    "\n",
    "def get_stream_from_x(path, size, specific_x):\n",
    "    count = 0\n",
    "    chunk_list = []\n",
    "    for chunk in pd.read_csv(path, chunksize=size, iterator=True):\n",
    "        if(count < specific_x):\n",
    "            print(count)\n",
    "            count+=1\n",
    "        else:\n",
    "            chunk_list.append(chunk)\n",
    "    df = pd.concat(chunk_list)\n",
    "    return df\n",
    "\n",
    "batch = get_stream_from_x(path='./train.csv', size=22000, specific_x=1)\n",
    "X_val, y = batch['Page content'], batch['Popularity']\n",
    "\n",
    "score = roc_auc_score(y, model.predict_proba(X_val)[:,1])\n",
    "\n",
    "print('validation: {}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('./test.csv')\n",
    "df_test_pred = pd.DataFrame(columns=['Id', 'Popularity'])\n",
    "X_test= df_test['Page content']\n",
    "\n",
    "y_pred = model.predict_proba(X_test)\n",
    "i = 0\n",
    "for y in y_pred:\n",
    "    print(df_test['Id'][i], y[1])\n",
    "    df_test_pred = df_test_pred.append({'Id': int(df_test['Id'][i]), 'Popularity':y[1]}, ignore_index=True)\n",
    "    i+=1\n",
    "df_test_pred['Id'] = df_test_pred['Id'].astype('int')\n",
    "df_test_pred.to_csv(\"./num_superXGB_ALL3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "54b8a75404ab4e6ba29309be618ca70d85c3d31f801fa89aae55b1caf37627da"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
