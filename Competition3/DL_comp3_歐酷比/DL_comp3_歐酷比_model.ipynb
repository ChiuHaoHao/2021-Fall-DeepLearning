{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"S983jlD634mD"},"outputs":[],"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","import tensorflow as tf\n","\n","from tensorflow.keras import layers\n","import os\n","\n","import string\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import PIL\n","import random\n","import time\n","from pathlib import Path\n","import imageio\n","import moviepy.editor as mpy\n","\n","import re\n","from IPython import display"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-B7JeDyo34mH","outputId":"b458d00b-907a-4f2f-d261-6d788f5cc012"},"outputs":[{"name":"stdout","output_type":"stream","text":["[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n","1 Physical GPUs, 1 Logical GPUs\n"]},{"name":"stderr","output_type":"stream","text":["2022-01-09 15:21:35.357000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-01-09 15:21:35.362658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-01-09 15:21:35.362988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-01-09 15:21:35.364556: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2022-01-09 15:21:35.365154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-01-09 15:21:35.365510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-01-09 15:21:35.365774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-01-09 15:21:35.756118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-01-09 15:21:35.756372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-01-09 15:21:35.756574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-01-09 15:21:35.756777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2889 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"]}],"source":["gpus = tf.config.experimental.list_physical_devices('GPU')\n","print(gpus)\n","if gpus:\n","    try:\n","        # Currently, memory growth needs to be the same across GPUs\n","        for gpu in gpus:\n","            tf.config.experimental.set_memory_growth(gpu, True)\n","        # Select GPU number 1\n","        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n","        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n","        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n","    except RuntimeError as e:\n","        # Memory growth must be set before GPUs have been initialized\n","        print(e)"]},{"cell_type":"markdown","metadata":{"id":"lWO_4--h34mJ"},"source":["# Load the training set we had preprocessed\n","* We used skip-throughs as our text embedding model.\n","* It can encode the whole sentence instead of a word, and it can predict next sentence and previous sentence. We chose it because of this property. \n","* Encode the discription to 4800 dimensions vector while given the text for condition. And the 4800 dimensions vector contains two part. The first 2400 is unidirectional vector, the other is bidirectional vecotr. We used first 2400 dimensions as our embedding."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zq8rh3gI34mL"},"outputs":[],"source":["st = np.load('./dataset/train_captions.npy', allow_pickle=True)"]},{"cell_type":"markdown","metadata":{"id":"rfHNWxuz34mL"},"source":["# Hyperparameter setting\n","* In this part, we used the setting in lab and competetion template"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zABtketC34mM"},"outputs":[],"source":["IMG_H = 64\n","IMG_W = 64\n","IMG_C = 3\n","IMG_SHAPE = (IMG_H, IMG_W, IMG_C)\n","\n","dataset_size = 7370\n","\n","Z_DIM = 128\n","text_dim = 128\n","BATCH_SIZE = 64\n","BZ = (BATCH_SIZE, 1, 1, 50)\n","\n","W_LR = 2.0e-04\n","W_beta1 = 0.5\n","W_beta2 = 0.99\n","W_EPOCH = 1500\n","\n","data_path = './dataset'\n","image_dir = ''\n","\n","hparas = {\n","    'MAX_SEQ_LENGTH': 20,                     # maximum sequence length\n","    'EMBED_DIM': 256,                         # word embedding dimension\n","    'RNN_HIDDEN_SIZE': 128,                   # number of RNN neurons\n","    'Z_DIM': 50,                             # random noise z dimension\n","    'DENSE_DIM': 128,                         # number of neurons in dense layer\n","    'IMAGE_SIZE': [64, 64, 3],                # render image size\n","    'BATCH_SIZE': 64,\n","    'LR': 1e-4,\n","    'LR_DECAY': 0.5,\n","    'BETA_1': 0.5,\n","    'N_EPOCH': 600,\n","    'CHECKPOINTS_DIR': './checkpoints/final',  \n","    'PRINT_FREQ': 1                           \n","}"]},{"cell_type":"markdown","metadata":{"id":"1AhI84lz34mN"},"source":["# Image preprocess\n","* We preprocessed the data by resizeing, cropping, padding, and random flip left and right.\n","* We generated the dataset which description didn't match it's image for the loss in conditional-GAN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZzCpY8ZX34mO"},"outputs":[],"source":["def image_preprocess(img):\n","    img = tf.image.decode_jpeg(img, channels=3)\n","    short_side = tf.minimum(tf.shape(img)[0], tf.shape(img)[1])\n","    img = tf.image.resize_with_crop_or_pad(img, short_side, short_side)\n","    img = tf.image.random_flip_left_right(img)\n","    img = tf.image.resize(img, [IMG_H, IMG_W])\n","    img = tf.cast(img, tf.float32)\n","    img = tf.clip_by_value(img, 0, 255)\n","    img = img/127.5 - 1.0\n","    return img\n","\n","def training_data_generator(captions, image_path, wrong_image_path):\n","    img = tf.io.read_file(image_dir+image_path)\n","    img = image_preprocess(img)\n","    wrong_img = tf.io.read_file(image_dir+wrong_image_path)\n","    wrong_img = image_preprocess(wrong_img)  \n","    caption = random.choice(captions)[:2400]\n","    return img, wrong_img, caption\n","\n","def dataset_generator(filenames, batch_size, data_generator):\n","    # load the training data into two NumPy arrays\n","    df = pd.read_pickle(filenames)\n","    real_image_path = df['ImagePath'].values\n","    wrong_image_path = tf.random.shuffle(real_image_path)\n","    \n","    dataset = tf.data.Dataset.from_tensor_slices((st, real_image_path, wrong_image_path))\n","    dataset = dataset.repeat(5)\n","    dataset = dataset.map(data_generator, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","    dataset = dataset.shuffle(5000).batch(batch_size, drop_remainder=True)\n","    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n","\n","    return dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m-pzt0Dr34mP","outputId":"3819aafd-a787-4d24-9f66-7880bccebffd"},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-01-09 15:21:49.237114: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 707520000 exceeds 10% of free system memory.\n","2022-01-09 15:21:49.484114: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 707520000 exceeds 10% of free system memory.\n"]}],"source":["dataset_train = dataset_generator(data_path + '/text2ImgData.pkl', BATCH_SIZE, training_data_generator)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xDJT8VA634mQ"},"outputs":[],"source":["class EmbeddingCompressor(tf.keras.Model):\n","    def __init__(self):\n","        super(EmbeddingCompressor, self).__init__()\n","        self.dense = tf.keras.layers.Dense(units = 128) # 128\n","\n","    def call(self, E):\n","        X = self.dense(E)\n","        return tf.nn.leaky_relu(X)"]},{"cell_type":"markdown","metadata":{"id":"C0LrZ07734mQ"},"source":["# Model Architecture\n","* We reference some papers or github pages for building our model.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LLrs7dYg34mR"},"outputs":[],"source":["class Generator(tf.keras.Model):\n","    \"\"\"\n","    Generate fake image based on given text(hidden representation) and noise z\n","    input: text and noise\n","    output: fake image with size 64*64*3\n","    \"\"\"\n","    def __init__(self, hparas):\n","        super(Generator, self).__init__()\n","        self.hparas = hparas\n","        self.flatten = tf.keras.layers.Flatten()\n","        self.d1 = EmbeddingCompressor() # DENSE_DIM = 128\n","        self.d2 = tf.keras.layers.Dense(units = 128*4*4*4, kernel_initializer = tf.random_normal_initializer(stddev = 0.02))\n","        self.reshape = tf.keras.layers.Reshape(target_shape = (4, 4, 128*4), input_shape = (128*4*4*4, ))\n","        self.batchnorm = tf.keras.layers.BatchNormalization()\n","        self.batchnorm0 = tf.keras.layers.BatchNormalization()\n","        self.batchnorm1 = tf.keras.layers.BatchNormalization()\n","        self.batchnorm2 = tf.keras.layers.BatchNormalization()\n","        self.batchnorm3 = tf.keras.layers.BatchNormalization()\n","        self.batchnorm4 = tf.keras.layers.BatchNormalization()\n","        self.batchnorm5 = tf.keras.layers.BatchNormalization()\n","        self.batchnorm7 = tf.keras.layers.BatchNormalization()\n","        self.batchnorm8 = tf.keras.layers.BatchNormalization()\n","        self.batchnorm6 = tf.keras.layers.BatchNormalization()\n","        self.conv_same1 = tf.keras.layers.Conv2D(filters = 256, kernel_size = 4, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal())\n","        self.conv_same2 = tf.keras.layers.Conv2D(filters = 256, kernel_size = 4, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal())\n","        self.conv_same3 = tf.keras.layers.Conv2D(filters = 256, kernel_size = 4, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal())\n","        self.conv_same4 = tf.keras.layers.Conv2D(filters = 256, kernel_size = 4, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal())\n","        self.conv_same5 = tf.keras.layers.Conv2D(filters = 64, kernel_size = 4, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal())\n","        self.conv_same6 = tf.keras.layers.Conv2D(filters = 128, kernel_size = 4, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal())\n","        self.conv_same7 = tf.keras.layers.Conv2D(filters = 128, kernel_size = 4, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal())\n","        self.conv_same8 = tf.keras.layers.Conv2D(filters = 64, kernel_size = 4, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal())\n","        self.reshape = tf.keras.layers.Reshape(target_shape = (4, 4, 128*4), input_shape = (128*4*4*4, ))\n","        self.deconv = tf.keras.layers.Conv2DTranspose(filters = 256, kernel_size = 4, strides = (2, 2), padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal())\n","        self.deconv0 = tf.keras.layers.Conv2DTranspose(filters = 256, kernel_size = 4, strides = (2, 2), padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal())\n","        self.deconv1 = tf.keras.layers.Conv2DTranspose(filters = 256, kernel_size = 4, strides = (2, 2), padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal())\n","        self.deconv2 = tf.keras.layers.Conv2DTranspose(filters = 128, kernel_size = 4, strides = (2, 2), padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal())\n","        self.deconv3 = tf.keras.layers.Conv2DTranspose(filters = 128, kernel_size = 4, strides = (2, 2), padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal())\n","        self.deconv4 = tf.keras.layers.Conv2DTranspose(filters = 3, kernel_size = 4, strides = (2, 2), padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal())\n","        self.conv1 = tf.keras.layers.Conv2D(filters = 256, kernel_size =1 , strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal())\n","        self.conv2 = tf.keras.layers.Conv2D(filters = 256, kernel_size = 1, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal())\n","        self.conv3 = tf.keras.layers.Conv2D(filters = 128, kernel_size = 1, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal())\n","        self.conv4 = tf.keras.layers.Conv2D(filters = 64, kernel_size = 1, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal())\n","    def call(self, text, noise_z):\n","        text = self.d1(text)\n","        \n","        text = tf.expand_dims(text, axis=1)\n","        text = tf.expand_dims(text, axis=1)\n","        \n","        noise_z = tf.expand_dims(noise_z, axis=1)\n","        noise_z = tf.expand_dims(noise_z, axis=1)\n","        \n","        img = tf.concat([text, noise_z], axis=3)\n","        X = self.deconv(img)\n","        X = self.batchnorm(X)\n","        X = tf.nn.leaky_relu(X)\n","        X = self.deconv0(X)\n","        X = self.batchnorm0(X)\n","        X = tf.nn.leaky_relu(X)\n","        \n","\n","        # ResBlock\n","        X = self.conv_same1(X)\n","        X = self.batchnorm1(X)\n","        img = self.conv1(img)\n","        X = tf.nn.leaky_relu(X)\n","        X = self.conv_same2(X)\n","        X = self.batchnorm2(X) + img\n","        X = tf.nn.leaky_relu(X)\n","\n","        # Up-sampling\n","        Res_X = self.deconv1(X)\n","        \n","        \n","        # ResBlock\n","        X = self.conv_same3(Res_X)\n","        X = self.batchnorm3(X)\n","        Res_X = self.conv2(Res_X)\n","        X = tf.nn.leaky_relu(X)\n","        X = self.conv_same4(X)\n","        X = self.batchnorm4(X) + Res_X\n","        X = tf.nn.leaky_relu(X)\n","\n","        # Up-sampling\n","        Res_X = self.deconv2(X)\n","        \n","        \n","        # ResBlock\n","        X = self.conv_same5(Res_X)\n","        X = self.batchnorm5(X)\n","        Res_X = self.conv3(Res_X)\n","        X = tf.nn.leaky_relu(X)\n","        X = self.conv_same6(X)\n","        X = self.batchnorm6(X) + Res_X\n","        X = tf.nn.leaky_relu(X)\n","\n","        # Up-sampling\n","        Res_X = self.deconv3(X)\n","        \n","        \n","        # ResBlock\n","        X = self.conv_same7(Res_X)\n","        X = self.batchnorm7(X)\n","        Res_X = self.conv4(Res_X)\n","        X = tf.nn.leaky_relu(X)\n","        X = self.conv_same8(X)\n","        X = self.batchnorm8(X) + Res_X\n","        X = tf.nn.leaky_relu(X)\n","\n","        # Up-sampling\n","        logits = self.deconv4(X)\n","        output = tf.nn.tanh(logits)\n","        \n","        return output"]},{"cell_type":"markdown","metadata":{"id":"9qUQcFv434mR"},"source":["### check if Generator works"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8LtbLs4k34mS","outputId":"127200c1-e4d8-40e2-c94e-fbf49468d39e"},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-01-09 15:21:52.654034: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n","2022-01-09 15:21:53.028090: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8202\n","2022-01-09 15:21:53.636254: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"]},{"data":{"text/plain":["<tf.Tensor: shape=(64, 64, 64, 3), dtype=float32, numpy=\n","array([[[[-0.5204494 , -0.94369143, -0.97567415],\n","         [-0.9999601 , -0.9901111 ,  0.9994086 ],\n","         [ 0.9954048 ,  0.9092694 ,  0.8924996 ],\n","         ...,\n","         [-0.54295564, -0.8364265 , -0.17171238],\n","         [ 0.24456996, -0.34253263, -0.9645204 ],\n","         [ 0.06219353,  0.6510829 ,  0.88143307]],\n","\n","        [[-0.9061176 , -0.8572843 , -0.9143423 ],\n","         [ 0.9999089 ,  0.9999567 ,  0.3664996 ],\n","         [-1.        , -0.99976164, -0.99915373],\n","         ...,\n","         [ 0.641505  , -0.08585563, -0.57915723],\n","         [ 0.8277271 , -0.71646   , -0.95273536],\n","         [ 0.9850715 , -0.10212433, -0.94293416]],\n","\n","        [[-0.9998918 , -0.9990797 , -1.        ],\n","         [ 1.        , -1.        ,  0.99997646],\n","         [ 0.6211959 , -0.99999946, -0.92935586],\n","         ...,\n","         [ 0.8714288 , -0.7560331 , -0.29461834],\n","         [ 0.9734517 ,  0.51597357, -0.9732179 ],\n","         [ 0.6578226 ,  0.87391704,  0.91657877]],\n","\n","        ...,\n","\n","        [[-0.9791557 , -0.8798894 ,  0.1829312 ],\n","         [ 0.98931605,  0.97728884,  0.619243  ],\n","         [-0.99813634,  0.95653427, -0.1403945 ],\n","         ...,\n","         [-0.13908537,  0.29374063, -0.67717004],\n","         [-0.5291588 ,  0.05737979, -0.20927949],\n","         [-0.47527954,  0.11441192,  0.13001174]],\n","\n","        [[-0.8243867 , -0.7103578 , -0.7290796 ],\n","         [ 0.9841643 , -0.98591393,  0.8808484 ],\n","         [ 0.9201868 , -0.9972231 , -0.98046213],\n","         ...,\n","         [ 0.32694304, -0.43317822, -0.08816101],\n","         [-0.17514509,  0.04446148, -0.4595232 ],\n","         [ 0.14929825,  0.4784128 ,  0.18299781]],\n","\n","        [[-0.7648518 ,  0.45882514,  0.4320992 ],\n","         [-0.4139035 ,  0.70368403, -0.03518758],\n","         [ 0.27539685,  0.86647373,  0.85795385],\n","         ...,\n","         [-0.04998063,  0.23069659,  0.03703196],\n","         [-0.04983014,  0.33749914,  0.08764941],\n","         [-0.04324299,  0.00491499, -0.0699346 ]]],\n","\n","\n","       [[[-0.3669736 , -0.92919976, -0.9317598 ],\n","         [-0.9999201 , -0.99766266,  0.9992521 ],\n","         [ 0.9934591 ,  0.91140515,  0.91426563],\n","         ...,\n","         [-0.55212206, -0.8680355 , -0.13549657],\n","         [ 0.3910034 , -0.40258816, -0.95985836],\n","         [-0.00843341,  0.6347616 ,  0.8567105 ]],\n","\n","        [[-0.78935975, -0.9371818 , -0.71359414],\n","         [ 0.9999353 ,  0.99999416, -0.15038913],\n","         [-1.        , -0.9995209 , -0.9700636 ],\n","         ...,\n","         [ 0.7156831 ,  0.15668665, -0.56269825],\n","         [ 0.7156757 , -0.7754232 , -0.966837  ],\n","         [ 0.9906131 , -0.04547997, -0.9451695 ]],\n","\n","        [[-0.9999296 , -0.99985254, -1.        ],\n","         [ 1.        , -1.        ,  0.9999848 ],\n","         [ 0.8262091 , -0.9999988 , -0.73376495],\n","         ...,\n","         [ 0.9022257 , -0.88344955, -0.44440702],\n","         [ 0.9549109 ,  0.44064027, -0.97704333],\n","         [ 0.76268613,  0.8933812 ,  0.9194785 ]],\n","\n","        ...,\n","\n","        [[-0.9905435 , -0.88474864,  0.25275242],\n","         [ 0.9809362 ,  0.9859256 ,  0.7038769 ],\n","         [-0.998742  ,  0.9683956 , -0.39298692],\n","         ...,\n","         [-0.08925004,  0.41992328, -0.68443465],\n","         [-0.5672596 ,  0.09484351, -0.27816877],\n","         [-0.46454826,  0.11473972,  0.13732861]],\n","\n","        [[-0.81627387, -0.6748393 , -0.78759915],\n","         [ 0.97804826, -0.9940238 ,  0.92649084],\n","         [ 0.8698854 , -0.987546  , -0.9905359 ],\n","         ...,\n","         [ 0.35999358, -0.39827496,  0.03643226],\n","         [-0.17193057,  0.06140097, -0.51012427],\n","         [ 0.12795398,  0.50740695,  0.17549187]],\n","\n","        [[-0.7601719 ,  0.48504543,  0.4453416 ],\n","         [-0.26729977,  0.7077279 , -0.29583636],\n","         [ 0.14515503,  0.86601996,  0.8515494 ],\n","         ...,\n","         [-0.07008183,  0.26293382,  0.02974661],\n","         [-0.09059743,  0.32445204,  0.10100731],\n","         [-0.04574458,  0.00913265, -0.05804291]]],\n","\n","\n","       [[[ 0.03739876, -0.9216945 , -0.9617462 ],\n","         [-0.99997985, -0.99709284,  0.99604344],\n","         [ 0.97327644,  0.90005094,  0.62966037],\n","         ...,\n","         [-0.5987231 , -0.8394324 , -0.10624965],\n","         [ 0.1942338 , -0.4883546 , -0.96098405],\n","         [-0.03582887,  0.6811896 ,  0.8812959 ]],\n","\n","        [[-0.9005332 , -0.8484934 , -0.9695662 ],\n","         [ 0.99980664,  0.9999962 , -0.27321237],\n","         [-1.        , -0.9996315 , -0.9997855 ],\n","         ...,\n","         [ 0.68847615, -0.01676007, -0.6391867 ],\n","         [ 0.7924009 , -0.6555971 , -0.9661341 ],\n","         [ 0.9858208 ,  0.03875975, -0.9424064 ]],\n","\n","        [[-0.9999424 , -0.9998752 , -1.        ],\n","         [ 1.        , -1.        ,  0.9999834 ],\n","         [-0.03788469, -0.99999976, -0.78233874],\n","         ...,\n","         [ 0.85812074, -0.7938294 , -0.29347947],\n","         [ 0.97799283,  0.56194985, -0.9627925 ],\n","         [ 0.69963557,  0.86262125,  0.9100509 ]],\n","\n","        ...,\n","\n","        [[-0.98916173, -0.85120404,  0.36018246],\n","         [ 0.9868541 ,  0.9745992 ,  0.73655206],\n","         [-0.99769086,  0.9719798 ,  0.08333258],\n","         ...,\n","         [-0.07656997,  0.25621337, -0.72988456],\n","         [-0.47243086,  0.11073805, -0.10326442],\n","         [-0.469685  ,  0.11656184,  0.15141214]],\n","\n","        [[-0.8356268 , -0.6509595 , -0.70056975],\n","         [ 0.9905925 , -0.9843053 ,  0.8678229 ],\n","         [ 0.93570614, -0.9974044 , -0.97796685],\n","         ...,\n","         [ 0.31727108, -0.34816936, -0.01278691],\n","         [-0.2309584 ,  0.13264367, -0.5188383 ],\n","         [ 0.17226715,  0.4558558 ,  0.19380185]],\n","\n","        [[-0.77946305,  0.4071476 ,  0.38844404],\n","         [-0.51745707,  0.70358425, -0.0927579 ],\n","         [ 0.26896578,  0.87385976,  0.7970985 ],\n","         ...,\n","         [-0.09554263,  0.19497593,  0.06208934],\n","         [-0.05741097,  0.32225752,  0.09157481],\n","         [-0.01423255, -0.01524653, -0.08750421]]],\n","\n","\n","       ...,\n","\n","\n","       [[[-0.30409908, -0.9742706 , -0.85961014],\n","         [-0.99995875, -0.99561584,  0.9979593 ],\n","         [ 0.9512842 ,  0.8408664 ,  0.3166031 ],\n","         ...,\n","         [-0.42078155, -0.83309096, -0.18655428],\n","         [ 0.12396754, -0.35977143, -0.9540325 ],\n","         [ 0.0853136 ,  0.56909704,  0.8605176 ]],\n","\n","        [[-0.93217605, -0.83238876, -0.9103583 ],\n","         [ 0.9996733 ,  0.9999957 ,  0.92724425],\n","         [-1.        , -0.99964976, -0.9980857 ],\n","         ...,\n","         [ 0.56803787, -0.13315594, -0.581777  ],\n","         [ 0.88326055, -0.77278197, -0.9594152 ],\n","         [ 0.9843864 , -0.1360755 , -0.9383097 ]],\n","\n","        [[-0.9997792 , -0.9996514 , -1.        ],\n","         [ 1.        , -1.        ,  0.99999404],\n","         [ 0.12325031, -0.9999991 , -0.8529275 ],\n","         ...,\n","         [ 0.91596025, -0.75845695, -0.56162494],\n","         [ 0.9622497 ,  0.22414802, -0.97310656],\n","         [ 0.70888907,  0.85900295,  0.89151055]],\n","\n","        ...,\n","\n","        [[-0.9917326 , -0.9002749 ,  0.03156352],\n","         [ 0.98618734,  0.98199505,  0.62393683],\n","         [-0.998589  ,  0.9369258 , -0.50932044],\n","         ...,\n","         [-0.06286766,  0.23845251, -0.7119935 ],\n","         [-0.54902184, -0.00704345, -0.11816487],\n","         [-0.4502118 ,  0.13110761,  0.1955443 ]],\n","\n","        [[-0.840073  , -0.6536946 , -0.7467303 ],\n","         [ 0.9837136 , -0.98770547,  0.91155523],\n","         [ 0.900622  , -0.99094343, -0.9854981 ],\n","         ...,\n","         [ 0.33169478, -0.49922734, -0.06398318],\n","         [-0.22610314,  0.02355685, -0.48663458],\n","         [ 0.16664885,  0.49321735,  0.18616787]],\n","\n","        [[-0.8081594 ,  0.4349618 ,  0.44867215],\n","         [-0.38156912,  0.74594814, -0.21737844],\n","         [ 0.11668508,  0.8666476 ,  0.8155227 ],\n","         ...,\n","         [-0.04965   ,  0.2627731 ,  0.03761043],\n","         [-0.07125071,  0.32878152,  0.04081786],\n","         [-0.03146251,  0.00986795, -0.07127137]]],\n","\n","\n","       [[[-0.38883203, -0.9319759 , -0.9364494 ],\n","         [-0.9999404 , -0.9990367 ,  0.99717176],\n","         [ 0.9839453 ,  0.88121766,  0.86752135],\n","         ...,\n","         [-0.6551472 , -0.8298709 , -0.18713851],\n","         [ 0.33460638, -0.29863963, -0.9614966 ],\n","         [ 0.03117319,  0.58370423,  0.8902709 ]],\n","\n","        [[-0.94154555, -0.75125355, -0.95420325],\n","         [ 0.9997086 ,  0.99999297,  0.810231  ],\n","         [-1.        , -0.9990209 , -0.9983985 ],\n","         ...,\n","         [ 0.64512676, -0.35663807, -0.55313015],\n","         [ 0.75088507, -0.73342   , -0.9555654 ],\n","         [ 0.9894959 , -0.09172693, -0.94022053]],\n","\n","        [[-0.9998335 , -0.99931085, -1.        ],\n","         [ 1.        , -1.        ,  0.9999866 ],\n","         [ 0.56219393, -0.9999992 , -0.9925708 ],\n","         ...,\n","         [ 0.8662148 , -0.7881127 , -0.4360861 ],\n","         [ 0.9690786 ,  0.2252326 , -0.98137915],\n","         [ 0.7662381 ,  0.8955269 ,  0.92799276]],\n","\n","        ...,\n","\n","        [[-0.9933275 , -0.87236977,  0.18966079],\n","         [ 0.9872036 ,  0.9762283 ,  0.84984535],\n","         [-0.9979724 ,  0.9698828 ,  0.04492316],\n","         ...,\n","         [-0.10581599,  0.33605748, -0.67952   ],\n","         [-0.46308884,  0.04316716, -0.14565447],\n","         [-0.46326765,  0.05061131,  0.12307717]],\n","\n","        [[-0.83650595, -0.707024  , -0.77307844],\n","         [ 0.9917902 , -0.97828597,  0.85730696],\n","         [ 0.89402705, -0.9960215 , -0.9846544 ],\n","         ...,\n","         [ 0.3997061 , -0.33059365, -0.04339807],\n","         [-0.1458714 ,  0.02513177, -0.58440393],\n","         [ 0.14311339,  0.47319886,  0.19983456]],\n","\n","        [[-0.8130297 ,  0.46158445,  0.38431865],\n","         [-0.49600416,  0.78566223, -0.18562463],\n","         [ 0.16668631,  0.9047426 ,  0.8032563 ],\n","         ...,\n","         [-0.08577105,  0.20297799,  0.04772614],\n","         [-0.02641593,  0.32413393,  0.09416805],\n","         [-0.04201471, -0.00545089, -0.0725834 ]]],\n","\n","\n","       [[[ 0.06936581, -0.93658656, -0.909022  ],\n","         [-0.9999653 , -0.9978599 ,  0.9967701 ],\n","         [ 0.995201  ,  0.800975  ,  0.77484876],\n","         ...,\n","         [-0.57195663, -0.85281795, -0.14430611],\n","         [ 0.2659761 , -0.35242608, -0.94034064],\n","         [ 0.0449243 ,  0.63103503,  0.8755687 ]],\n","\n","        [[-0.9436814 , -0.83933634, -0.87786514],\n","         [ 0.9999394 ,  0.9999905 ,  0.43136024],\n","         [-1.        , -0.99977314, -0.9965313 ],\n","         ...,\n","         [ 0.66626465, -0.03506139, -0.34579822],\n","         [ 0.88015586, -0.5518586 , -0.93470037],\n","         [ 0.9840173 , -0.16491926, -0.9246037 ]],\n","\n","        [[-0.99998057, -0.9999423 , -1.        ],\n","         [ 1.        , -1.        ,  0.9999847 ],\n","         [ 0.8871807 , -1.        , -0.45917726],\n","         ...,\n","         [ 0.937248  , -0.79879165, -0.17878741],\n","         [ 0.9712943 ,  0.54695815, -0.9731542 ],\n","         [ 0.6241187 ,  0.881145  ,  0.90852433]],\n","\n","        ...,\n","\n","        [[-0.9901492 , -0.83697855,  0.20125389],\n","         [ 0.98084706,  0.98461276,  0.75599974],\n","         [-0.9979969 ,  0.9726825 , -0.17042081],\n","         ...,\n","         [-0.09740663,  0.30716714, -0.73825514],\n","         [-0.5127876 ,  0.12751731, -0.11680029],\n","         [-0.4402306 ,  0.0719043 ,  0.1412228 ]],\n","\n","        [[-0.8280561 , -0.75086874, -0.7216959 ],\n","         [ 0.9802763 , -0.9806666 ,  0.8683676 ],\n","         [ 0.8788569 , -0.99309725, -0.97281   ],\n","         ...,\n","         [ 0.353739  , -0.3503378 , -0.01434755],\n","         [-0.21271896,  0.1296671 , -0.52746767],\n","         [ 0.15410179,  0.4511865 ,  0.22372116]],\n","\n","        [[-0.7675089 ,  0.4116251 ,  0.55150926],\n","         [-0.27066037,  0.6520891 , -0.05875828],\n","         [ 0.1234545 ,  0.8462769 ,  0.8460147 ],\n","         ...,\n","         [-0.08853473,  0.1879595 ,  0.05651806],\n","         [-0.10642871,  0.3486377 ,  0.12566985],\n","         [-0.02568273, -0.00754417, -0.06068729]]]], dtype=float32)>"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["noise = tf.random.uniform(shape=[64,100], minval=0.7, maxval=1.)\n","text = tf.random.uniform(shape=[64,100], minval=0.7, maxval=1.)\n","g = Generator(hparas)\n","g(text, noise)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HvklVOOT34mS"},"outputs":[],"source":["class Discriminator(tf.keras.Model):\n","    \"\"\"\n","    Differentiate the real and fake image\n","    input: image and corresponding text\n","    output: labels, the real image should be 1, while the fake should be 0\n","    \"\"\"\n","    def __init__(self, hparas):\n","        super(Discriminator, self).__init__()\n","        self.hparas = hparas\n","        self.flatten = tf.keras.layers.Flatten()\n","        self.batch_norm1 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n","        self.batch_norm2 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n","        self.batch_norm3 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n","        self.batch_norm4 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n","        self.batch_norm5 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n","        self.batch_norm6 = tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)\n","        self.conv1 = tf.keras.layers.Conv2D(filters = 16, kernel_size =1 , strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal())\n","        self.ap_1 =  tf.keras.layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')\n","        self.conv2 = tf.keras.layers.Conv2D(filters = 32, kernel_size = 1, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal())\n","        self.ap_2 =  tf.keras.layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')\n","        self.conv3 = tf.keras.layers.Conv2D(filters = 64, kernel_size = 1, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal())\n","        self.ap_3 =  tf.keras.layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')\n","        self.conv4 = tf.keras.layers.Conv2D(filters = 128, kernel_size = 1, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal())\n","        self.ap_4 =  tf.keras.layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')\n","        self.conv5 = tf.keras.layers.Conv2D(filters = 128, kernel_size = 1, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal())\n","        self.conv6 = tf.keras.layers.Conv2D(filters = 128, kernel_size = 1, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal())\n","        \n","        \n","        self.conv_same1 = tf.keras.layers.Conv2D(filters = 16, kernel_size = 4, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal())\n","        self.conv_same2 = tf.keras.layers.Conv2D(filters = 16, kernel_size = 4, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal())\n","        self.conv_same3 = tf.keras.layers.Conv2D(filters = 32, kernel_size = 4, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal())\n","        self.conv_same4 = tf.keras.layers.Conv2D(filters = 32, kernel_size = 4, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal())\n","        self.conv_same5 = tf.keras.layers.Conv2D(filters = 64, kernel_size = 4, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal())\n","        self.conv_same6 = tf.keras.layers.Conv2D(filters = 64, kernel_size = 4, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal())\n","        self.conv_same7 = tf.keras.layers.Conv2D(filters = 128, kernel_size = 4, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal())\n","        self.conv_same8 = tf.keras.layers.Conv2D(filters = 128, kernel_size = 4, strides = 1, padding = \"same\", kernel_initializer = tf.keras.initializers.TruncatedNormal())\n","        \n","        self.d_img = tf.keras.layers.Dense(self.hparas['DENSE_DIM'])\n","\n","        self.embed = tf.keras.layers.Dense(256) # for text\n","        self.reshape = tf.keras.layers.Reshape(target_shape = (1, 1, 128))\n","        self.concat = tf.keras.layers.Concatenate()\n","        self.last_d1 = tf.keras.layers.Dense(128)\n","        self.last_d2 = tf.keras.layers.Dense(1)\n","        \n","    def call(self, text, img, train=None):\n","        \n","        \n","        # Res_Block\n","        \n","        X = self.conv_same1(img)\n","        X = self.batch_norm1(X)\n","        img = self.conv1(img)\n","        X = tf.nn.leaky_relu(X)\n","        X = self.conv_same2(X)\n","        X = self.batch_norm2(X) + img\n","        X = tf.nn.leaky_relu(X)\n","        Res_X = self.ap_1(X)\n","        \n","        # Res_Block\n","        X = self.conv_same3(Res_X)\n","        X = self.batch_norm3(X)\n","        Res_X = self.conv2(Res_X)\n","        X = tf.nn.leaky_relu(X)\n","        X = self.conv_same4(X)\n","        X = self.batch_norm4(X) + Res_X\n","        X = tf.nn.leaky_relu(X)\n","        Res_X = self.ap_2(X)\n","        \n","        \n","        # Res_Block\n","        X = self.conv_same5(Res_X)\n","        X = self.batch_norm5(X)\n","        Res_X = self.conv3(Res_X)\n","        X = tf.nn.leaky_relu(X)\n","        X = self.conv_same6(X)\n","        X = self.batch_norm6(X) + Res_X\n","        X = tf.nn.leaky_relu(X)\n","        Res_X = self.ap_3(X)\n","        \n","    \n","        \n","        T = self.embed(text)\n","        T = tf.reshape(T, [64,16,16,1])\n","        X = tf.concat([X, T], 3)\n","        \n","        merged_input = self.conv5(X, training=train)\n","        merged_input = self.conv6(merged_input, training=train)\n","        \n","        merged_input = self.flatten(merged_input)\n","        \n","\n","        Y = self.last_d1(merged_input)\n","        logits = self.last_d2(Y)\n","        output = tf.nn.sigmoid(logits)\n","        # return logits, output\n","        return logits"]},{"cell_type":"markdown","metadata":{"id":"Z3ScjJEW34mT"},"source":["### check if Discriminator works"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"09KaXVag34mT","outputId":"94efef41-5328-45b5-ab29-8a0831a7334f"},"outputs":[{"data":{"text/plain":["<tf.Tensor: shape=(64, 1), dtype=float32, numpy=\n","array([[-0.0437666 ],\n","       [-0.05221966],\n","       [-0.05009417],\n","       [-0.05511851],\n","       [-0.05305054],\n","       [-0.04867206],\n","       [-0.04897459],\n","       [-0.05139726],\n","       [-0.05004559],\n","       [-0.04404794],\n","       [-0.05481909],\n","       [-0.05086534],\n","       [-0.05123526],\n","       [-0.04773366],\n","       [-0.0520039 ],\n","       [-0.04918405],\n","       [-0.04656748],\n","       [-0.05110708],\n","       [-0.04592313],\n","       [-0.04780387],\n","       [-0.05596517],\n","       [-0.05304431],\n","       [-0.04863382],\n","       [-0.04825912],\n","       [-0.04492773],\n","       [-0.05068965],\n","       [-0.05201664],\n","       [-0.05120134],\n","       [-0.04668717],\n","       [-0.04745018],\n","       [-0.0508514 ],\n","       [-0.04915348],\n","       [-0.05349309],\n","       [-0.04925626],\n","       [-0.04406099],\n","       [-0.04861335],\n","       [-0.04834228],\n","       [-0.05456967],\n","       [-0.04869805],\n","       [-0.04834382],\n","       [-0.04847126],\n","       [-0.04867705],\n","       [-0.05287009],\n","       [-0.0496944 ],\n","       [-0.04844553],\n","       [-0.05121548],\n","       [-0.05274714],\n","       [-0.05018098],\n","       [-0.05266584],\n","       [-0.04759282],\n","       [-0.04329928],\n","       [-0.04975325],\n","       [-0.05030902],\n","       [-0.05192786],\n","       [-0.04876203],\n","       [-0.04836733],\n","       [-0.04916059],\n","       [-0.05119197],\n","       [-0.04638161],\n","       [-0.0483772 ],\n","       [-0.05275116],\n","       [-0.05033553],\n","       [-0.04824347],\n","       [-0.05646965]], dtype=float32)>"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["img = tf.random.uniform(shape=[64,64,64,3], minval=0.7, maxval=1.)\n","text = tf.random.uniform(shape=[64,100], minval=0.7, maxval=1.)\n","g = Discriminator(hparas)\n","g(text, img)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O8_RSvt434mU"},"outputs":[],"source":["generator = Generator(hparas)\n","discriminator = Discriminator(hparas)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1rHaUmRY34mU"},"outputs":[],"source":["optimizer_g = tf.keras.optimizers.Adam(W_LR, beta_1=W_beta1, beta_2=W_beta2)\n","optimizer_d = tf.keras.optimizers.Adam(W_LR, beta_1=W_beta1, beta_2=W_beta2)"]},{"cell_type":"markdown","metadata":{"id":"SAUGEnXW34mU"},"source":["# Loss\n","* In this part, we used the W-GAN loss in the lab for our training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JVNVRYU434mU"},"outputs":[],"source":["cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits = True)\n","\n","def DC_G_Loss(z0):\n","    return cross_entropy(tf.ones_like(z0), z0)\n","\n","def DC_D_Loss(z0, z_caption, z1):\n","    l0 = cross_entropy(tf.zeros_like(z0), z0)\n","    l1 = cross_entropy(tf.ones_like(z1), z1)\n","    l_caption = cross_entropy(tf.zeros_like(z_caption), z_caption)\n","    return l0, l_caption, l1\n","\n","@tf.function\n","def W_GTrain(real_img, wrong_img, text):\n","    z = tf.random.normal(shape=[hparas['BATCH_SIZE'], hparas['Z_DIM']], mean=0.0, stddev=1.0)\n","    with tf.GradientTape() as tpg:\n","        c0 = generator(text, z, training = True)\n","        z1 = discriminator(text, real_img, training = True)\n","        z0 = discriminator(text, c0, training = True)\n","        z_caption = discriminator(text, wrong_img, training = True)\n","        lg = DC_G_Loss(z0)\n","        l0, l_caption, l1 = DC_D_Loss(z0, z1, z_caption)\n","        ld = l0/2.0+ l_caption + l1\n","    gradient_g = tpg.gradient(lg, generator.trainable_variables)\n","    optimizer_g.apply_gradients(zip(gradient_g, generator.trainable_variables))\n","    \n","    return lg, (l0, l_caption, l1)\n","\n","@tf.function\n","def W_DTrain(real_img, wrong_img, text):\n","    z = tf.random.normal(shape=[hparas['BATCH_SIZE'], hparas['Z_DIM']], mean=0.0, stddev=1.0)\n","    with tf.GradientTape() as tpd:\n","        c0 = generator(text, z, training = True)\n","    \n","\n","        z1 = discriminator(text, real_img, training = True)\n","        z0 = discriminator(text, c0, training = True)\n","        z_caption = discriminator(text, wrong_img, training = True)\n","\n","        lg = DC_G_Loss(z0)\n","        \n","        l0, l_caption, l1 = DC_D_Loss(z0, z1, z_caption)\n","        ld = l0/2.0+ l_caption + l1\n","\n","    gradient_d = tpd.gradient(ld, discriminator.trainable_variables)\n","\n","    optimizer_d.apply_gradients(zip(gradient_d, discriminator.trainable_variables))\n","    \n","    return lg, (l0, l_caption, l1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zoeRw7wQ34mV"},"outputs":[],"source":["WTrain = (\n","    W_DTrain,\n","    W_DTrain,\n","    W_DTrain,\n","    W_GTrain\n",")\n","\n","WCritic = len(WTrain)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d7f3W6V434mV"},"outputs":[],"source":["@tf.function\n","def test_step(caption):\n","    noise = tf.random.normal(shape=[hparas['BATCH_SIZE'], hparas['Z_DIM']], mean=0.0, stddev=1.0)\n","    fake_image = generator(caption, noise, training=False)\n","    return fake_image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ApIEfjps34mV"},"outputs":[],"source":["def merge(images, size):\n","    h, w = images.shape[1], images.shape[2]\n","    img = np.zeros((h * size[0], w * size[1], 3))\n","    for idx, image in enumerate(images):\n","        i = idx % size[1]\n","        j = idx // size[1]\n","        img[j*h:j*h+h, i*w:i*w+w, :] = image\n","    return img\n","\n","def imsave(images, size, path):\n","    # getting the pixel values between [0, 1] to save it\n","    return plt.imsave(path, merge(images, size)*0.5 + 0.5)\n","\n","def save_images(images, size, image_path):\n","    return imsave(images, size, image_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6CPRDZkD34mW"},"outputs":[],"source":["def sample_generator(captions_test):\n","    captions_test = np.asarray(captions_test)\n","    dataset = tf.data.Dataset.from_tensor_slices(captions_test)\n","    dataset = dataset.batch(BATCH_SIZE)\n","    return dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Zupv0v734mW","outputId":"d0f30f3c-20ac-4bd6-8fe9-7b4bd29961ce"},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'sample_captions.npy'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_7785/1557114636.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mst_check\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sample_captions.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m~/anaconda3/envs/DL/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sample_captions.npy'"]}],"source":["st_check = np.load('sample_captions.npy')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qUvJtryH34mW"},"outputs":[],"source":["ni = int(np.ceil(np.sqrt(BATCH_SIZE)))\n","sample_size = BATCH_SIZE\n","sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, 1, 1, Z_DIM)).astype(np.float32)\n","sample_sentence = [st_check[1][0][:2400]] * int(sample_size/ni) + \\\n","                  [st_check[2][0][:2400]] * int(sample_size/ni) + \\\n","                  [st_check[3][0][:2400]] * int(sample_size/ni) + \\\n","                  [st_check[4][0][:2400]] * int(sample_size/ni) + \\\n","                  [st_check[5][0][:2400]] * int(sample_size/ni) + \\\n","                  [st_check[6][0][:2400]] * int(sample_size/ni) + \\\n","                  [st_check[7][0][:2400]] * int(sample_size/ni) +\\\n","                  [st_check[7][0][:2400]] * int(sample_size/ni)\n","\n","sample_sentence = sample_generator(sample_sentence)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XuWlPz9y34mW"},"outputs":[],"source":["ckpt = tf.train.Checkpoint(optimizer_g=optimizer_g,\n","                           optimizer_d=optimizer_d,\n","                           generator=generator,\n","                           discriminator=discriminator)\n","\n","manager = tf.train.CheckpointManager(ckpt, './checkpoints/final', max_to_keep=1500)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Caj7LZPo34mW"},"outputs":[],"source":["wlg = [None] * W_EPOCH #record loss of g for each epoch\n","wld = [None] * W_EPOCH #record loss of d for each epoch\n","wsp = [None] * W_EPOCH #record sample images for each epoch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ktDuTE0Y34mX"},"outputs":[],"source":["# Utility function\n","def utPuzzle(imgs, row, col, path=None):\n","    h, w, c = imgs[0].shape\n","    out = np.zeros((h * row, w * col, c), np.uint8)\n","    for n, img in enumerate(imgs):\n","        j, i = divmod(n, col)\n","        out[j * h : (j + 1) * h, i * w : (i + 1) * w, :] = img\n","    if path is not None : imageio.imwrite(path, out)\n","    return out\n","  \n","def utMakeGif(imgs, fname, duration):\n","    n = float(len(imgs)) / duration\n","    clip = mpy.VideoClip(lambda t : imgs[int(n * t)], duration = duration)\n","    clip.write_gif(fname, fps = n)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sIeNXKjp34mX"},"outputs":[],"source":["# ckpt.restore('/home/haowei/CS565600_Deep_Learning/DL_comp3/checkpoints/chiu_80_ver6mergeOu' + '/ckpt-732')"]},{"cell_type":"markdown","metadata":{"id":"SssKM_2r34mX"},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Z63rhUp34mX"},"outputs":[],"source":["rsTrain = float(BATCH_SIZE) / (float(dataset_size) * 5)\n","ctr = 0\n","\n","g_1 = []\n","g_2 = []\n","g_3 = []\n","wlg = []\n","for ep in range(1, W_EPOCH):\n","    print('start epoch {}'.format(ep))\n","    start = time.time()\n","\n","    lgt = 0.0\n","    ldt = 0.0\n","    l0, l_caption, l1 = 0.0, 0.0, 0.0\n","    idx = 0\n","    for img, wrong_img, caption in dataset_train:\n","        idx+=1\n","        lg, ld = WTrain[ctr](img, wrong_img, caption)\n","        ctr += 1\n","        l0 += ld[0].numpy()\n","        l_caption += ld[1].numpy()\n","        l1 += ld[2].numpy()\n","        lgt += lg.numpy()\n","        if ctr == WCritic : ctr = 0\n","\n","    wlg[ep] = lgt * rsTrain\n","    wld[ep] = ldt * rsTrain\n","    g_1.append(l0 * rsTrain)\n","    g_2.append(l_caption * rsTrain)\n","    g_3.append(l1 * rsTrain)\n","    wlg.append(lgt * rsTrain)\n","    \n","\n","    print('\\rEnd epoch {}, lg = {:.4f}, ld = ((l0 = {:.4f} + l_caption = {:.4f})/2 + l1 = {:.4f})'.format(ep, lgt * rsTrain, l0 * rsTrain,l_caption * rsTrain,l1 * rsTrain))\n","    print('Time for epoch {} is {:.4f} sec'.format(ep, time.time()-start))\n","    \n","    if (ep + 1) % 2 == 0:\n","        y_label = range(0,len(g_1))\n","        plt.plot(y_label, g_1, 'g', label='l0')\n","        plt.plot(y_label, g_2, 'b', label='l_caption')\n","        plt.plot(y_label, g_3, 'r', label='11')\n","        plt.plot(y_label, wlg, 'y', label='G_loss')\n","        wlg\n","        plt.title('Training and Validation loss')\n","        plt.xlabel('Epochs')\n","        plt.ylabel('Loss')\n","        plt.legend()\n","        plt.show()\n","        save_path = manager.save()\n","        print(\"Saved checkpoint for epoch {}: {}\".format(ep, save_path))\n","\n","    # visualization\n","    if (ep + 1) % 1 == 0:\n","        for caption in sample_sentence:\n","            fake_image = test_step(caption)\n","        save_images(fake_image, [ni, ni], './samples/demo_80_ver6mergeOu/train_{:04d}.jpg'.format(ep))\n","        fake_image = utPuzzle(\n","            ((fake_image + 1)*127.5).numpy().astype(np.uint8),\n","            8,\n","            8\n","        )\n","        wsp[ep] = fake_image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jl9VRAR234mX"},"outputs":[],"source":["def testing_data_generator(captions, index):\n","    caption = captions[0][:2400]\n","    return caption, index\n","\n","def testing_dataset_generator(batch_size, data_generator):\n","    data = pd.read_pickle('./dataset/testData.pkl')\n","    st = np.load('./test_captions.npy')\n","    index = data['ID'].values\n","    index = np.asarray(index)\n","    \n","    dataset = tf.data.Dataset.from_tensor_slices((st, index))\n","    dataset = dataset.map(data_generator, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","    dataset = dataset.repeat().batch(batch_size)\n","    \n","    return dataset\n","\n","testing_dataset = testing_dataset_generator(BATCH_SIZE, testing_data_generator)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MPvOnmoQ34mY"},"outputs":[],"source":["data = pd.read_pickle('./dataset/testData.pkl')\n","captions = data['Captions'].values\n","\n","NUM_TEST = len(captions)\n","EPOCH_TEST = int(NUM_TEST / hparas['BATCH_SIZE'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MmZZ8b4t34mY"},"outputs":[],"source":["@tf.function\n","def test_step(caption, noise):\n","    text_embed = caption\n","    fake_image = generator(text_embed, noise)\n","    return fake_image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MqEaO1hy34mY"},"outputs":[],"source":["def inference(dataset):\n","    sample_size = hparas['BATCH_SIZE']\n","    sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, hparas['Z_DIM'])).astype(np.float32)\n","    \n","    step = 0\n","    start = time.time()\n","    for captions, idx in dataset:\n","        if step > EPOCH_TEST:\n","            break\n","        \n","        fake_image = test_step(captions, sample_seed)\n","        step += 1\n","        for i in range(hparas['BATCH_SIZE']):\n","            plt.imsave('./inference/chiu_80_ver6mergeOu_checker_new_propotion/inference_{:04d}.jpg'.format(idx[i]), fake_image[i].numpy()*0.5 + 0.5)\n","            \n","    print('Time for inference is {:.4f} sec'.format(time.time()-start))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LRLMUhW834mY"},"outputs":[],"source":["inference(testing_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OKVH4DzB34mY"},"outputs":[],"source":["def visualize(idx):\n","    fig = plt.figure(figsize=(14, 14))\n","    \n","    for count, i in enumerate(idx):\n","        loc = np.where(i==index)[0][0]\n","        text = ''\n","        for word in captions[loc]:\n","            if id2word_dict[word] != '<PAD>':\n","                text += id2word_dict[word]\n","                text += ' '\n","        print(text)\n","        \n","        path = './inference/chiu_80_ver6mergeOu_checker_new_propotion/inference_{:04d}.jpg'.format(i)\n","        fake_iamge = plt.imread(path)\n","        \n","        plt.subplot(7, 7, count+1)\n","        plt.imshow(fake_iamge)\n","        plt.axis('off')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UVcQ5GPu34mZ"},"outputs":[],"source":["dictionary_path = './dictionary'\n","vocab = np.load(dictionary_path + '/vocab.npy')\n","print('there are {} vocabularies in total'.format(len(vocab)))\n","\n","word2Id_dict = dict(np.load(dictionary_path + '/word2Id.npy'))\n","id2word_dict = dict(np.load(dictionary_path + '/id2Word.npy'))\n","print('Word to id mapping, for example: %s -> %s' % ('flower', word2Id_dict['flower']))\n","print('Id to word mapping, for example: %s -> %s' % ('1', id2word_dict['1']))\n","print('Tokens: <PAD>: %s; <RARE>: %s' % (word2Id_dict['<PAD>'], word2Id_dict['<RARE>']))"]},{"cell_type":"markdown","metadata":{"id":"KN5iQHP134mZ"},"source":["# Inference the result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BGBygPmG34mZ"},"outputs":[],"source":["data = pd.read_pickle('./dataset/testData.pkl')\n","captions = data['Captions'].values\n","index = data['ID'].values\n","random_idx = [23, 216, 224, 413, 713, 859, 876, 974, 1177, 1179, 1241, 2169, 2196, 2237, \n","              2356, 2611, 2621, 2786, 2951, 2962, 3145, 3255, 3327, 3639, 3654, 3927, 4262, \n","              4321, 4517, 5067, 5147, 5955, 6167, 6216, 6410, 6413, 6579, 6584, 6804, 6988, \n","              7049, 7160]\n","\n","print(len(random_idx))\n","visualize(random_idx)"]},{"cell_type":"markdown","metadata":{"id":"EcmOvP5g34mZ"},"source":["# Report"]},{"cell_type":"markdown","metadata":{"id":"68u-NqfM34mZ"},"source":["## Models you tried during competition"]},{"cell_type":"markdown","metadata":{"id":"FONbayWX34mZ"},"source":["### Text encoder:\n","We tried three kinds of model for text encoding, GRU, Bert, and Skip-thoughts. And we got 0.64, 0.63, 0.61 on public score respectively.\n","Finally, we chose Skip-thoughts as our text encoder. It needed to download the pretrian model and weight from github(https://github.com/ryankiros/skip-thoughts), and fed the condition sentence sentences to get the embedding and transformed the result to .npy file. \n","\n","### GAN:\n","We used the original conditional GAN as our baseline, we follow the paper to bulid our model. But we noticed the \"Condition Augmentation\" which can add some noise to the given embedding will fool the generator to generate the image mismatch to the sentence, so we remove it, we guessed it was due to our small dataset. After we passed the baseline60, we want to try StackGAN, but we noticed that it would take many time when training stage2, so we gave up. Maybe next time we should start our project as soon as possible. :-(\n","\n"]},{"cell_type":"markdown","metadata":{"id":"01XRMHSc34mZ"},"source":["## List the experiment you did"]},{"cell_type":"markdown","metadata":{"id":"nH8eAoXh34ma"},"source":["### Data augmentation\n","We use resize_with_crop_or_pad and random_flip_left_right to augment the original image first, after that we size it to 64*64.\n","\n","### Hyper-parameters tuning\n","For the hyper parameter tuning, we just used the origin setting in the lab and task.\n","\n","### Architecture tuning\n","For the \"Generator\", we tried original conditional GAN as our baseline, we follow the paper to bulid our model. But we noticed the \"Condition Augmentation\" which can add some noise to the given embedding will fool the generator to generate the image mismatch to the sentence, so we remove it. After that, we find some paper used residual block to help training, so we add it to our model. As for the \"Discriminator\", we also added more layers when the generated image mismatch to the sentence because we thought this circumstance in result from the poor discriminator.\n","\n","### Optimizer tuning\n","For the training optimizer, we only used Adam as our optimizer because most of the paper used this way. And for each training step, we trained the discriminator for three times and generator for one time."]},{"cell_type":"markdown","metadata":{"id":"b4xRLiKJ34ma"},"source":["# Anything worth mentioning\n","\n","### Skip-thoughts V.S. Bert\n","We find some difference between these two encoders when we applied them to our task. For Bert pretrain on Imagenet, because it had learned too many words before, it may embedded our sentence in similar sequence. This result would make our sentence mismatch to the image because the generator couldn't recognize the little difference between them. As for Skip-thoughts, it can encode the whole sentence instead of a word, and it can predict next sentence and previous sentence. It can make the generated image more match to the given sentence. However, the image generated by the embedded from Bert was more like real flowers. So for the competition, we used one result from Bert but we train the discriminator more times and add more layers to it. And the others versions we used the skip-thoughts as text-encoder.\n","\n","###  Training step and loss\n","We used the training step in the template fist, however, we notice that it is unreasonable because it update the generator and discriminator's optimizer in the same time, so we modified the training step similar to lab sample, we train and update the discriminator for 3 times and the generator for 1 times respectively. We also used the wgan loss to train it and got better result on public score. For the loss, beside (real image, caption), (fake image, caption), we follow the paper and add a new loss pair (real image, mismatch caption) when counting generator loss."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PBBdZh6134ma"},"outputs":[],"source":[""]}],"metadata":{"interpreter":{"hash":"582a547d56616ed5ded758bd597d037ed2282210a9e90cbdefccd6d7e4528f9e"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.11"},"colab":{"name":"DL_comp3_歐酷比_model.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}